import streamlit as st
import pandas as pd
import openai
from sentence_transformers import SentenceTransformer
import numpy as np
import re
import requests
import json
import nest_asyncio
import asyncio
from ekonlpy.tag import Mecab  # eKoNLPy ì‚¬ìš©

nest_asyncio.apply()

try:
    asyncio.get_running_loop()
except RuntimeError:
    asyncio.set_event_loop(asyncio.new_event_loop())

# ê¹ƒí—ˆë¸Œ ì €ì¥ì†Œ ì •ë³´
owner = "junslee96"
repo = "Bang_boardgame_chatbot"
file_path = "prompt_data"

merged_data_url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/{file_path}/merged_data.json"
output_data_url = f"https://raw.githubusercontent.com/{owner}/{repo}/main/{file_path}/output_data.json"


def load_data():
    try:
        response_merged = requests.get(merged_data_url)
        if response_merged.status_code == 200:
            merged_data = response_merged.json()
        else:
            print(f"Failed to read merged data. Status code: {response_merged.status_code}")
            return None

        response_qa = requests.get(output_data_url)
        if response_qa.status_code == 200:
            qa_data = response_qa.json()
            qa_df = pd.DataFrame(qa_data)
        else:
            print(f"Failed to read QA data. Status code: {response_qa.status_code}")
            qa_df = None

        # ë‘ ë°ì´í„°ì…‹ì„ í†µí•©í•˜ì—¬ ë¬¸ì„œ ìƒì„±
        documents = []
        for item in merged_data:
            if 'content' in item:
                documents.append(item['content'])

        if qa_df is not None:
            for _, row in qa_df.iterrows():
                # ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ í†µí•©
                documents.append(f"ì§ˆë¬¸: {row['ì§ˆë¬¸']}\në‹µë³€: {row['ë‹µë³€']}")

        return documents
    except Exception as e:
        print(f"Error loading data: {e}")
        return None


def chunk_text(text, chunk_size=200):
    words = text.split()
    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]


def vectorize_documents(documents):
    model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')
    chunked_documents = []
    for doc in documents:
        chunked_documents.extend(chunk_text(doc))
    X = model.encode(chunked_documents)
    return chunked_documents, X


def retrieve_similar_documents(query, documents, X, top_k=3):
    try:
        model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')
        query_vec = model.encode([query])

        similarities = np.dot(X, query_vec.T[0]).flatten()
        top_indices = similarities.argsort()[-top_k:][::-1]

        chunk_to_doc_map = {}
        chunk_index = 0
        for doc in documents:
            chunked_doc = chunk_text(doc)
            for _ in chunked_doc:
                chunk_to_doc_map[chunk_index] = doc
                chunk_index += 1

        top_docs = []
        for idx in top_indices:
            if idx in chunk_to_doc_map:
                top_docs.append(chunk_to_doc_map[idx])

        return top_docs

    except Exception as e:
        print(f"Error in retrieve_similar_documents: {e}")
        return []


def create_context(retrieved_docs):
    mecab = Mecab()
    stop_words = set(['ë¥¼', 'ì„', 'ëŠ”', 'ì´', 'ê°€', 'ì—', 'ì™€', 'ê³¼', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ê¹Œì§€'])

    relevant_sentences = []
    for doc in retrieved_docs:
        sentences = doc.split('. ')
        for sentence in sentences:
            morphs = mecab.morphs(sentence)
            if len(morphs) > 10 and not any(morph in stop_words for morph in morphs):
                relevant_sentences.append(sentence)

    sentence_scores = []
    for sentence in relevant_sentences:
        score = len([morph for morph in mecab.morphs(sentence) if morph not in stop_words])
        sentence_scores.append((sentence, score))

    top_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)[:5]
    return '\n'.join([sentence for sentence, _ in top_sentences])


def generate_response(query, conversation_history, persona_profile):
    if "chunked_documents" not in st.session_state or "X" not in st.session_state:
        return "í˜„ì¬ ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•´ì£¼ì„¸ìš”."

    retrieved_docs = retrieve_similar_documents(query, st.session_state["chunked_documents"], st.session_state["X"])
    context = create_context(retrieved_docs)
    answer_prompt = f"{persona_profile}\n\n{conversation_history}\n\nì§ˆë¬¸: {query}\në‹µë³€:"

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "assistant", "content": answer_prompt}
        ],
        max_tokens=1000,
        temperature=0.2,
        top_p=0.01,
        frequency_penalty=1.2,
        stream=False
    )

    answer = response.choices[0].message.content
    return answer


def replace_terms(text):
    replace_dict = {'ì‚¬ëŒ': 'í”Œë ˆì´ì–´'}
    for key, value in replace_dict.items():
        text = re.sub(key, value, text)
    return text


def transform_query(query):
    transformed_query = query + " ê´€ë ¨ ì •ë³´"
    return transformed_query


st.title("ğŸ¤  ë±… ë³´ë“œê²Œì„ ì±—ë´‡")
st.write(
    "OpenAIì˜ gpt-4o-mini ëª¨ë¸ì„ ì‚¬ìš©í•´ì„œ ë§Œë“  ê°„ë‹¨í•œ ìƒì„±í˜• ì±—ë´‡ì…ë‹ˆë‹¤."
)

openai_api_key = st.text_input("OpenAI API Key", type="password")
if not openai_api_key:
    st.info("Please add your OpenAI API key to continue.", icon="ğŸ—ï¸")
else:
    client = openai.OpenAI(api_key=openai_api_key)

if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "documents" not in st.session_state:
    st.session_state["documents"] = load_data() or []

if "chunked_documents" not in st.session_state or "X" not in st.session_state:
    if st.session_state["documents"]:
        st.session_state["chunked_documents"], st.session_state["X"] = vectorize_documents(st.session_state["documents"])
    else:
        st.session_state["chunked_documents"] = []
        st.session_state["X"] = []


persona_profile = """
ì´ë¦„: ë±…! ë³´ë“œê²Œì„ ê°€ì´ë“œ
ë‚˜ì´: 25-40ì„¸
ê´€ì‹¬ì‚¬: ì„œë¶€ ì‹œëŒ€, ë³´ë“œê²Œì„, ì „ëµ, í˜‘ë™
ì—­í• : ë³´ì•ˆê´€, ë¶€ê´€, ë¬´ë²•ì, ë°°ì‹ ì
ëª©í‘œ: ê²Œì„ì—ì„œ ìŠ¹ë¦¬í•˜ê¸° ìœ„í•´ ì—­í• ì— ë§ëŠ” ëª©í‘œë¥¼ ë‹¬ì„±
íŠ¹ì„±: ì „ëµì ì´ê³ , ìƒí™©ì— ë§ê²Œ ì ì‘í•˜ë©°, ë•Œë¡œëŠ” ìœ„í—˜ì„ ê°ìˆ˜í•˜ê¸°ë„ í•¨

ì˜ˆì‹œ ëŒ€í™”:
"ì•ˆë…•í•˜ì„¸ìš”! ë±…! ë³´ë“œê²Œì„ì„ ì‹œì‘í•˜ê¸° ì „ì—..."
"""

for message in st.session_state["messages"]:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("What is up?"):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)

    modified_question = replace_terms(prompt)

    try:
        conversation_history = '\n'.join([f"{msg['role']}: {msg['content']}" for msg in st.session_state["messages"]])
        
        answer = generate_response(modified_question, conversation_history, persona_profile)
        
        st.session_state["messages"].append({"role": "assistant", "content": answer})
        
        with st.chat_message("assistant"):
            st.markdown(answer)

    except Exception as e:
        st.error(f"An error occurred while generating a response: {e}")
